{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nB7rq9FaRml_"
   },
   "source": [
    "#Intro to audio data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "\n",
    "array, sampling_rate = librosa.load(librosa.ex(\"trumpet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Type array: {type(array)}, type sampling rate: {sampling_rate}\\n\\nArray size: {array.shape}, Sampling rate = {sampling_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see above that array is 1D denoting samples of audio signal. The example is loaded as a tuple of audio time series.\n",
    "\n",
    "print(f\"Lenght of audio: {array.size/sampling_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "\n",
    "\n",
    "plt.figure().set_figwidth(12)\n",
    "librosa.display.waveshow(array, sr=sampling_rate)  #amplitude of the signal on the y-axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qriNbzkKaDu-"
   },
   "source": [
    "##The frequency spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "dft_input = array[:4096]\n",
    "\n",
    "# calculate the DFT\n",
    "window = np.hanning(len(dft_input))\n",
    "windowed_input = dft_input * window\n",
    "dft = np.fft.rfft(windowed_input)\n",
    "\n",
    "# get the amplitude spectrum in decibels\n",
    "amplitude = np.abs(dft)\n",
    "amplitude_db = librosa.amplitude_to_db(amplitude, ref=np.max)\n",
    "\n",
    "# get the frequency bins\n",
    "frequency = librosa.fft_frequencies(sr=sampling_rate, n_fft=len(dft_input))\n",
    "\n",
    "#This plots the strength of the various frequency components that are present in this audio segment\n",
    "\n",
    "plt.figure().set_figwidth(12)\n",
    "plt.plot(frequency, amplitude_db)\n",
    "plt.xlabel(\"Frequency (Hz)\")\n",
    "plt.ylabel(\"Amplitude (dB)\")\n",
    "plt.xscale(\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JOansXxCcMq6"
   },
   "source": [
    "The output of the DFT is an array of complex numbers, made up of real and imaginary components. Taking the magnitude with np.abs(dft) extracts the amplitude information from the spectrogram. The angle between the real and imaginary components provides the so-called phase spectrum, but this is often discarded in machine learning applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FUFN5x8yfFs0"
   },
   "source": [
    "##Spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "D = librosa.stft(array)\n",
    "S_db = librosa.amplitude_to_db(np.abs(D), ref=np.max)\n",
    "\n",
    "plt.figure().set_figwidth(12)\n",
    "librosa.display.specshow(S_db, x_axis=\"time\", y_axis=\"hz\")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KhQxvjY3i3oU"
   },
   "source": [
    "##Mel spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = librosa.feature.melspectrogram(y=array, sr=sampling_rate, n_mels=128, fmax=8000)\n",
    "S_dB = librosa.power_to_db(S, ref=np.max)\n",
    "\n",
    "plt.figure().set_figwidth(12)\n",
    "librosa.display.specshow(S_dB, x_axis=\"time\", y_axis=\"mel\", sr=sampling_rate, fmax=8000)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xaq_1EtJjrxT"
   },
   "source": [
    "In the example above, n_mels stands for the number of mel bands to generate. The mel bands define a set of frequency ranges that divide the spectrum into perceptually meaningful components, using a set of filters whose shape and spacing are chosen to mimic the way the human ear responds to different frequencies. Common values for n_mels are 40 or 80. fmax indicates the highest frequency (in Hz) we care about."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m1q73NWZkBNn"
   },
   "source": [
    "Creating a mel spectrogram is a lossy operation as it involves filtering the signal. Converting a mel spectrogram back into a waveform is more difficult than doing this for a regular spectrogram, as it requires estimating the frequencies that were thrown away. This is why machine learning models such as HiFiGAN vocoder are needed to produce a waveform from a mel spectrogram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YmLAq83eRe9p"
   },
   "source": [
    "#Load and explore an audio dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets[audio]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "minds = load_dataset(\"PolyAI/minds14\", name=\"en-AU\", split=\"train\") #To load MINDS-14 dataset, use datasetâ€™s identifier on the Hub (PolyAI/minds14). We are choosing Australian subset (en-AU) of the data, and limit it to the training split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Dataset contains: {minds}\\n\\nType: {type(minds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The shape of array: {(minds[0]['audio']['array']).shape}, sampling rate: {minds[0]['audio']['sampling_rate']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"output class in classification task: {minds.unique('intent_class')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert these number into a meaningful string, we can use the int2str() method\n",
    "\n",
    "id2label = minds.features[\"intent_class\"].int2str\n",
    "\n",
    "{class_int: id2label(class_int) for class_int in sorted(minds.unique('intent_class'))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove irrelevant features using Datasets remove_columns method\n",
    "\n",
    "columns_to_remove = [\"lang_id\", \"english_transcription\"]\n",
    "minds = minds.remove_columns(columns_to_remove)\n",
    "minds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Listen to audio example\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "\n",
    "def generate_audio():\n",
    "    example = minds.shuffle()[0]\n",
    "    audio = example[\"audio\"]\n",
    "    return (\n",
    "        audio[\"sampling_rate\"],\n",
    "        audio[\"array\"],\n",
    "    ), id2label(example[\"intent_class\"])\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Column():\n",
    "        for _ in range(4):\n",
    "            audio, label = generate_audio()\n",
    "            output = gr.Audio(audio, label=label)\n",
    "\n",
    "demo.launch(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "\n",
    "example = minds[0]\n",
    "array = example[\"audio\"][\"array\"]\n",
    "sampling_rate = example[\"audio\"][\"sampling_rate\"]\n",
    "\n",
    "plt.figure().set_figwidth(12)\n",
    "librosa.display.waveshow(array, sr=sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1VdHURpYhoVN"
   },
   "source": [
    "#Preprocessing an audio dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZX8PYR3nhtjZ"
   },
   "source": [
    "It involves the following steps:\n",
    "\n",
    "1.   Resampling the audio data\n",
    "2.   Filtering the dataset\n",
    "3.   Converting audio data to modelâ€™s expected input\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resampling\n",
    "\n",
    "from datasets import Audio\n",
    "\n",
    "minds = minds.cast_column(\"audio\", Audio(sampling_rate=16_000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The shape of array: {(minds[0]['audio']['array']).shape}, sampling rate: {minds[0]['audio']['sampling_rate']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7yl4hrIBjLnJ"
   },
   "source": [
    "**Filtering the dataset**\n",
    "\n",
    "We may need to filter the data based on some criteria. One of the common cases involves limiting the audio examples to a certain duration. For instance, we might want to filter out any examples longer than 20s to prevent **out-of-memory errors** when training a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_DURATION_IN_SECONDS = 20.0\n",
    "\n",
    "\n",
    "def is_audio_length_in_range(input_length):\n",
    "    return input_length < MAX_DURATION_IN_SECONDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use librosa to get example's duration from the audio file\n",
    "new_column = [librosa.get_duration(path=x) for x in minds[\"path\"]]\n",
    "minds = minds.add_column(\"duration\", new_column)\n",
    "\n",
    "# use Datasets' `filter` method to apply the filtering function\n",
    "minds = minds.filter(is_audio_length_in_range, input_columns=[\"duration\"])\n",
    "\n",
    "# remove the temporary helper column\n",
    "minds = minds.remove_columns([\"duration\"])\n",
    "minds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U36oQ5b9q8-M"
   },
   "source": [
    "###Pre-processing audio data\n",
    "\n",
    "Convert raw data into input features.\n",
    "\n",
    "The requirements for the input features may vary from one model to another â€” they depend on the modelâ€™s architecture, and the data it was pre-trained with. The good news is, for every supported audio model, Transformers (library) offer a feature extractor class that can convert raw audio data into the input features the model expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the feature extractor from the pre-trained Whisper checkpoint to have ready for our audio data:\n",
    "\n",
    "from transformers import WhisperFeatureExtractor\n",
    "\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess single audio example\n",
    "\n",
    "def prepare_dataset(example):\n",
    "    audio = example[\"audio\"]\n",
    "    features = feature_extractor(\n",
    "        audio[\"array\"], sampling_rate=audio[\"sampling_rate\"], padding=True\n",
    "    )\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minds = minds.map(prepare_dataset)\n",
    "minds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "example = minds[0]\n",
    "input_features = example[\"input_features\"]\n",
    "\n",
    "plt.figure().set_figwidth(12)\n",
    "librosa.display.specshow(\n",
    "    np.asarray(input_features[0]),\n",
    "    x_axis=\"time\",\n",
    "    y_axis=\"mel\",\n",
    "    sr=feature_extractor.sampling_rate,\n",
    "    hop_length=feature_extractor.hop_length,\n",
    ")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ItpPrxfs-Gd"
   },
   "source": [
    "The modelâ€™s feature extractor class takes care of transforming raw audio data to the format that the model expects. However, many tasks involving audio are multimodal, e.g. speech recognition. In such cases Transformers (library) also offer model-specific tokenizers to process the text inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DrMNuXsGtLKH"
   },
   "source": [
    "We can load the feature extractor and tokenizer for Whisper and other multimodal models separately, or you can load both via a so-called processor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"openai/whisper-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i01EAya5vLKi"
   },
   "source": [
    "##Stream audio data\n",
    "\n",
    "ðŸ”Š Audio datasets can be huge (GBs to TBs), making full downloads impractical.\n",
    "\n",
    "âœ… HuggingFace Datasets offers `streaming=True` to load data sample-by-sample without downloading the full dataset.\n",
    "\n",
    "ðŸš€ Benefits: no disk space required, faster experimentation, and immediate data access.\n",
    "\n",
    "âš ï¸ Caveat: streamed data isn't cached, so repeated access requires re-streaming.\n",
    "\n",
    "ðŸ‘‰ Use: The only difference is that you can no longer access individual samples using Python indexing (i.e. `gigaspeech[\"train\"][sample_idx]`). Instead, you have to iterate over the dataset. Hereâ€™s how you can access an example when streaming a dataset: `next(iter(gigaspeech[\"train\"]))`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gigaspeech = load_dataset(\"speechcolab/gigaspeech\", \"xs\", streaming=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
